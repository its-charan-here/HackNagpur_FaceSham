{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":["import os, sys, time\n","import cv2\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["test_dir = \"./input/deepfake-detection-challenge/own_test/\"\n","\n","test_videos = sorted([x for x in os.listdir(test_dir) if x[-4:] == \".mp4\"])\n","frame_h = 5\n","frame_l = 5\n","len(test_videos)"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['_a_charan.mp4',\n"," '_deepest_fake.mp4',\n"," '_result-charan1.mp4',\n"," '_vig.mp4',\n"," 'all.mp4',\n"," 'mohit.mp4',\n"," 'panchal.mp4']"]},"metadata":{},"execution_count":2}],"source":["test_videos"]},{"metadata":{"trusted":true},"cell_type":"code","source":[],"execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["print(\"PyTorch version:\", torch.__version__)\n","print(\"CUDA version:\", torch.version.cuda)\n","print(\"cuDNN version:\", torch.backends.cudnn.version())"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch version: 1.6.0\nCUDA version: 10.1\ncuDNN version: 7604\n"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":["gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(gpu)"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":6}]},{"metadata":{"trusted":true},"cell_type":"code","source":["import sys\n","sys.path.insert(0, \"/input/blazeface-pytorch\")\n","sys.path.insert(0, \"/input/deepfakes-inference-demo\")"],"execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["from blazeface import BlazeFace\n","facedet = BlazeFace().to(gpu)\n","facedet.load_weights(r\"input\\blazeface-pytorch\\blazeface.pth\")\n","facedet.load_anchors(r\"input\\blazeface-pytorch\\anchors.npy\")\n","_ = facedet.train(False)"],"execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["from helpers.read_video_1 import VideoReader\n","from helpers.face_extract_1 import FaceExtractor\n","\n","frames_per_video = 81 #frame_h * frame_l\n","video_reader = VideoReader()\n","video_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\n","face_extractor = FaceExtractor(video_read_fn, facedet)"],"execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["input_size = 480"],"execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["from torchvision.transforms import Normalize\n","\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","normalize_transform = Normalize(mean, std)"],"execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def isotropically_resize_image(img, size, resample=cv2.INTER_AREA):\n","    h, w = img.shape[:2]\n","    if w > h:\n","        h = h * size // w\n","        w = size\n","    else:\n","        w = w * size // h\n","        h = size\n","\n","    resized = cv2.resize(img, (w, h), interpolation=resample)\n","    return resized\n","\n","\n","def make_square_image(img):\n","    h, w = img.shape[:2]\n","    size = max(h, w)\n","    t = 0\n","    b = size - h\n","    l = 0\n","    r = size - w\n","    return cv2.copyMakeBorder(img, t, b, l, r, cv2.BORDER_CONSTANT, value=0)"],"execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["import torch.nn as nn\n","import torchvision.models as models\n","\n","class MyResNeXt(models.resnet.ResNet):\n","    def __init__(self, training=True):\n","        super(MyResNeXt, self).__init__(block=models.resnet.Bottleneck,\n","                                        layers=[3, 4, 6, 3], \n","                                        groups=32, \n","                                        width_per_group=4)\n","        self.fc = nn.Linear(2048, 1)"],"execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["checkpoint = torch.load(r\"input\\deepfakes-inferece-demo\\resnext.pth\", map_location=gpu)\n","\n","model = MyResNeXt().to(gpu)\n","model.load_state_dict(checkpoint)\n","_ = model.eval()\n","\n","del checkpoint"],"execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def predict_on_video(video_path, batch_size):\n","    try:\n","        # Find the faces for N frames in the video.\n","        faces = face_extractor.process_video(video_path)\n","        # print(faces)\n","\n","        # Only look at one face per frame.\n","        face_extractor.keep_only_best_face(faces)\n","        \n","        if len(faces) > 0:\n","            # NOTE: When running on the CPU, the batch size must be fixed\n","            # or else memory usage will blow up. (Bug in PyTorch?)\n","            x = np.zeros((batch_size, input_size, input_size, 3), dtype=np.uint8)\n","\n","            # If we found any faces, prepare them for the model.\n","            n = 0\n","            for frame_data in faces:\n","                for face in frame_data[\"faces\"]:\n","                    # Resize to the model's required input size.\n","                    # We keep the aspect ratio intact and add zero\n","                    # padding if necessary.                    \n","                    resized_face = isotropically_resize_image(face, input_size)\n","                    resized_face = make_square_image(resized_face)\n","\n","                    if n < batch_size:\n","                        x[n] = resized_face\n","                        n += 1\n","                    else:\n","                        print(\"WARNING: have %d faces but batch size is %d\" % (n, batch_size))\n","                    \n","                    # Test time augmentation: horizontal flips.\n","                    # TODO: not sure yet if this helps or not\n","                    #x[n] = cv2.flip(resized_face, 1)\n","                    #n += 1\n","\n","            if n > 0:\n","                x = torch.tensor(x, device=gpu).float()\n","\n","                # Preprocess the images.\n","                x = x.permute((0, 3, 1, 2))\n","\n","                for i in range(len(x)):\n","                    x[i] = normalize_transform(x[i] / 255.)\n","\n","                # Make a prediction, then take the average.\n","                with torch.no_grad():\n","                    y_pred = model(x)\n","                    y_pred = torch.sigmoid(y_pred.squeeze())\n","                    return y_pred[:n].mean().item()\n","\n","    except Exception as e:\n","        print(\"Prediction error on video %s: %s\" % (video_path, str(e)))\n","\n","    return 0.5"],"execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["from concurrent.futures import ThreadPoolExecutor\n","\n","def predict_on_video_set(videos, num_workers):\n","    def process_file(i):\n","        filename = videos[i]\n","        y_pred = predict_on_video(os.path.join(test_dir, filename), batch_size=frames_per_video)\n","        return y_pred\n","\n","    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n","        predictions = ex.map(process_file, range(len(videos)))\n","\n","    return list(predictions)"],"execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["speed_test = True  # you have to enable this manually"],"execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# train_videos"],"execution_count":23,"outputs":[]},{"metadata":{"trusted":true,"tags":[]},"cell_type":"code","source":["if speed_test:\n","    start_time = time.time()\n","    speedtest_videos = test_videos[:5]\n","    predictions = predict_on_video_set(speedtest_videos, num_workers=4)\n","    elapsed = time.time() - start_time\n","    print(\"Elapsed %f sec. Average per video: %f sec.\" % (elapsed, elapsed / len(speedtest_videos)))"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Elapsed 41.435339 sec. Average per video: 8.287068 sec.\n"]}]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# y_pred = predict_on_img(r\"E:\\My Documents\\charan.jpg\", batch_size=1)"]},{"metadata":{"trusted":true},"cell_type":"code","source":["speedtest_videos    "],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['_a_charan.mp4',\n"," '_deepest_fake.mp4',\n"," '_result-charan1.mp4',\n"," '_vig.mp4',\n"," 'all.mp4']"]},"metadata":{},"execution_count":26}]},{"metadata":{"trusted":true},"cell_type":"code","source":["predictions"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.533103883266449,\n"," 0.4669482111930847,\n"," 0.6251113414764404,\n"," 0.05502110347151756,\n"," 0.4842233657836914]"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['_a_charan.mp4',\n"," '_deepest_fake.mp4',\n"," '_result-charan1.mp4',\n"," '_vig.mp4',\n"," 'all.mp4',\n"," 'mohit.mp4',\n"," 'panchal.mp4']"]},"metadata":{},"execution_count":28}],"source":["test_videos"]},{"metadata":{"trusted":true,"tags":[]},"cell_type":"code","source":["predictions = predict_on_video_set(test_videos, num_workers=4)"],"execution_count":29,"outputs":[]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.533103883266449,\n"," 0.4669482111930847,\n"," 0.6251113414764404,\n"," 0.05502110347151756,\n"," 0.4842233657836914,\n"," 0.6406271457672119,\n"," 0.47697553038597107]"]},"metadata":{},"execution_count":30}],"source":["predictions"]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.8.3-final","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}